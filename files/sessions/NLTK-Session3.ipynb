{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['UDS2013680-447-full.txt', 'UDS2013680-610-full.txt', 'UDS2013680-385-full.txt']\n"
     ]
    }
   ],
   "source": [
    "files=os.listdir('./UMA_Fraser_Radio_Talks/')\n",
    "print files [:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "speech=open('./UMA_Fraser_Radio_Talks/UDS2013680-151-full.txt','r').read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "tokens=word_tokenize(speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!--start metadata-->\r\n",
      "Title: Re-organisation of the Australian Meat Board\r\n",
      "Description: press statement\r\n",
      "Date: 19/04/1964\r\n",
      "Collection: John Malcolm F\n"
     ]
    }
   ],
   "source": [
    "print speech[:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 76),\n",
       " ('.', 40),\n",
       " ('of', 33),\n",
       " (',', 32),\n",
       " ('Board', 24),\n",
       " ('a', 21),\n",
       " ('Australian', 20),\n",
       " ('and', 20),\n",
       " ('will', 19),\n",
       " ('to', 18),\n",
       " ('The', 15),\n",
       " ('be', 15),\n",
       " ('is', 14),\n",
       " ('meat', 11),\n",
       " ('have', 11),\n",
       " ('Meat', 11),\n",
       " ('in', 11),\n",
       " ('from', 10),\n",
       " ('for', 10),\n",
       " ('on', 10)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist1=nltk.FreqDist(tokens)\n",
    "fdist1.most_common()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1081\n"
     ]
    }
   ],
   "source": [
    "print len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "419\n"
     ]
    }
   ],
   "source": [
    "print len(set(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "939\n",
      "353\n"
     ]
    }
   ],
   "source": [
    "vocab=[word for word in tokens if word.isalpha()] #just words no punctuation signs\n",
    "print len(vocab)\n",
    "words2=[word.lower() for word in vocab ] #strip out upper case words\n",
    "print len(set(words2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "291\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "ignored_words=nltk.corpus.stopwords.words('english')\n",
    "words3=[word for word in words2 if word not in ignored_words] #striping out stopwords\n",
    "print len(set(words3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('board', 25),\n",
       " ('meat', 22),\n",
       " ('australian', 21),\n",
       " ('committee', 12),\n",
       " ('selection', 8),\n",
       " ('members', 7),\n",
       " ('research', 7),\n",
       " ('government', 6),\n",
       " ('representatives', 6),\n",
       " ('new', 6),\n",
       " ('producers', 6),\n",
       " ('industry', 6),\n",
       " ('wool', 5),\n",
       " ('old', 5),\n",
       " ('conference', 5),\n",
       " ('council', 5),\n",
       " ('beef', 4),\n",
       " ('markets', 4),\n",
       " ('minister', 4),\n",
       " ('also', 4)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist2=nltk.FreqDist(words3)\n",
    "fdist2.most_common()[:20] #different result from fdist1 without stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 'The'),\n",
       " ('Australian', 'Meat'),\n",
       " ('Board', '.'),\n",
       " ('Meat', 'Board'),\n",
       " ('Selection', 'Committee'),\n",
       " ('of', 'the'),\n",
       " ('on', 'the'),\n",
       " ('the', 'Australian'),\n",
       " ('to', 'the'),\n",
       " ('will', 'be')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ngrams group of words that occur together Bigram : Two words\n",
    "from nltk.collocations import * \n",
    "bigram_measures=nltk.collocations.BigramAssocMeasures()\n",
    "finder=BigramCollocationFinder.from_words(tokens)\n",
    "sorted(finder.nbest(bigram_measures.raw_freq,10)) #Statistical Measure to find the best fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('australian', 'meat'),\n",
       " ('australian', 'wool'),\n",
       " ('board', 'selection'),\n",
       " ('four', 'members'),\n",
       " ('industry', 'conference'),\n",
       " ('meat', 'board'),\n",
       " ('meat', 'producers'),\n",
       " ('new', 'board'),\n",
       " ('selection', 'committee'),\n",
       " ('wool', 'industry')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ngrams group of words that occur together Bigram : Two words\n",
    "from nltk.collocations import * \n",
    "bigram_measures=nltk.collocations.BigramAssocMeasures()\n",
    "finder=BigramCollocationFinder.from_words(words3) #now with words3 without stopwords\n",
    "sorted(finder.nbest(bigram_measures.raw_freq,10)) #Statistical Measure to find the best fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n",
      "['The', 'family', 'of', 'Dashwood', 'had', 'long', 'been', 'settled', 'in', 'Sussex', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.book import * \n",
    "print sent2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nltk.download('all')# download shed load of text\n",
    "from nltk.util import ngrams\n",
    "n=3 #trigrams - 3 words to be together\n",
    "trigrams=ngrams(sent2,3)\n",
    "for gram in trigrams:\n",
    "    print gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The', 'family', 'of')\n",
      "('family', 'of', 'Dashwood')\n",
      "('of', 'Dashwood', 'had')\n",
      "('Dashwood', 'had', 'long')\n",
      "('had', 'long', 'been')\n",
      "('long', 'been', 'settled')\n",
      "('been', 'settled', 'in')\n",
      "('settled', 'in', 'Sussex')\n",
      "('in', 'Sussex', '.')\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "n=3 #trigrams - 3 words to be together\n",
    "trigrams=ngrams(sent2,n)\n",
    "for gram in trigrams:\n",
    "    print gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict \n",
    "def ngrammer(text,gramsize,threshold=4):\n",
    "    def list_duplicates(seq):\n",
    "        tally=defaultdict(list)\n",
    "        for i,item in enumerate(seq):\n",
    "            tally[item].append(i)\n",
    "        return ((len(locs),key) for key, locs in tally.items() if len(locs)> threshold)\n",
    "    raw_grams=ngrams(text,gramsize)\n",
    "    dupes=list_duplicates(raw_grams)\n",
    "    return sorted(dupes,reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(9, ('the', 'Australian', 'Meat')),\n",
       " (7, ('Australian', 'Meat', 'Board')),\n",
       " (5, ('Selection', 'Committee', '.')),\n",
       " (5, ('Board', 'Selection', 'Committee')),\n",
       " (4, ('the', 'Board', '.')),\n",
       " (4, (',', 'together', 'with'))]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrammer(tokens,3,threshold=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(7, ('the', 'Australian', 'Meat', 'Board')),\n",
       " (4, ('Board', 'Selection', 'Committee', '.'))]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrammer(tokens,4,threshold=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'list'>\n",
      "<class 'nltk.text.Text'>\n",
      "Selection Committee; Australian Meat; Meat Board; new Board; four\n",
      "members; private traders; Board Selection; Australian Wool; Primary\n",
      "Producers; Federal Council; Industry Conference; Wool Industry; nine\n",
      "members; Meat Exporters; producer representatives; Australian Primary;\n",
      "beef research; meat producers; Australian meat\n"
     ]
    }
   ],
   "source": [
    "print type(tokens)\n",
    "text=nltk.Text(tokens)\n",
    "print type(text)\n",
    "text.collocations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "from urllib import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿The Project Gutenberg EBook of Cocoa and Chocolate, by Arthur W. Knapp\r\n",
      "\r\n",
      "This eBook is for the u\n"
     ]
    }
   ],
   "source": [
    "cocoa=urlopen('http://gutenberg.digitalfabulists.org/pg19073.txt').read() #download file from internet COCOA & Chocolate\n",
    "print cocoa[:100] #first 100 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(cocoa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Cocoa and Chocolate\n",
      "﻿The Project Gutenberg EBook of Cocoa and Chocolate, by Arthur W. Knapp\r\n",
      "\r\n",
      "This eBook is for the use of anyone anywhere at no cost and with\r\n",
      "almost no restrictions whatsoever.  You may copy it, give i\n"
     ]
    }
   ],
   "source": [
    "raw=unicode(cocoa,'utf-8')\n",
    "title=next(line for line in raw.splitlines() if line.startswith('Title'))\n",
    "print title\n",
    "print raw[:200] #first 200 characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/researcher\r\n"
     ]
    }
   ],
   "source": [
    "!pwd #run a shell command\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/researcher/books/books/books\n",
      " created file Title: Cocoa and Chocolate\n"
     ]
    }
   ],
   "source": [
    "#command script with python. To do things in bulk\n",
    "!mkdir books\n",
    "%cd books\n",
    "filetittle=title.replace(' ','-')\n",
    "bookfile=open(filetittle,'w')\n",
    "bookfile.write(raw.encode('utf-8'))\n",
    "bookfile.close()\n",
    "print 'created file',title\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied (use --upgrade to upgrade): beautifulsoup4 in /opt/python/lib/python2.7/site-packages\r\n"
     ]
    }
   ],
   "source": [
    "#install beautifulsoup4 in system\n",
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup #import Beautiful Soup to strip out format and focus mainly on the content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'str'>\n",
      "e>Synthetic biology - Wikipedia, the free encyclopedia</title>\n",
      "<script>document.documentElement.className = document.documentElement.className.replace( /(^|\\s)client-nojs(\\s|$)/, \"$1client-js$2\" );</script>\n",
      "<script>window.RLQ = window.RLQ || []; window.RLQ.push( function () {\n",
      "mw.config.set({\"wgCanon\n"
     ]
    }
   ],
   "source": [
    "url='https://en.wikipedia.org/wiki/Synthetic_biology'\n",
    "raw=urlopen(url).read()\n",
    "print type(raw)\n",
    "print raw[100:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.BeautifulSoup'>\n"
     ]
    }
   ],
   "source": [
    "soup=BeautifulSoup(raw,'html.parser')\n",
    "print type(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'[1]Synthetic biology is an interdisciplinary branch of biology, combining disciplines such as biotechnology, evolutionary biology, molecular biology, systems biology, biophysics, computer engineering, and genetic engineering.', u'The definition of synthetic biology is debated not only among natural scientists but also in the human sciences, arts and politics.[2] One popular definition[3] is \"designing and constructing biological devices,[4] biological systems, and biological machines for useful purposes.\" However, the functional aspects of this definition stem from molecular biology and biotechnology.[5]', u'', u'', u'The term \"synthetic biology\" has a history spanning the twentieth century. The first use was in St\\xe9phane Leduc\\u2019s publication of \\xab\\xa0Th\\xe9orie physico-chimique de la vie et g\\xe9n\\xe9rations spontan\\xe9es\\xa0\\xbb (1910)[6] and \\xab\\xa0La Biologie Synth\\xe9tique\\xa0\\xbb (1912).[7][who said this?] In 1974, the Polish geneticist Wac\\u0142aw Szybalski used the term \"synthetic biology\",[8] writing:']\n"
     ]
    }
   ],
   "source": [
    "texts=[]\n",
    "for para in soup.find_all('p'):\n",
    "    text=para.text\n",
    "    texts.append(text)\n",
    "print texts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "texts #too big - clear cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'unicode'>\n",
      "Synthetic biology is an interdisciplinary branch of biology, combining disciplines such as biotechnology, evolutionary biology, molecular biology, systems biology, biophysics, computer engineering, an\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "regex=re.compile('\\[[0-9]*\\]')\n",
    "joined_texts='\\n'.join(texts)\n",
    "joined_texts=re.sub(regex, '',joined_texts)\n",
    "print type(joined_texts)\n",
    "print joined_texts[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/researcher/books/books/books\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/researcher/books\n"
     ]
    }
   ],
   "source": [
    "%cd ../../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "synbio_tokens=nltk.word_tokenize(joined_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created file synbiotokens.txt\n"
     ]
    }
   ],
   "source": [
    "synbio_tokens=nltk.word_tokenize(joined_texts) #e acute\n",
    "filetittle='synbiotokens.txt'\n",
    "bookfile=open(filetittle,'w')\n",
    "bookfile.write(joined_texts.encode('utf-8'))\n",
    "bookfile.close()\n",
    "print 'created file',filetittle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use was in Stéphane \n"
     ]
    }
   ],
   "source": [
    "print joined_texts[680:700] #e acute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'\\nGuardian sport', u'\\n\\nWednesday 18 November 2015 11.14 AEDT\\n\\n\\nLast modified on Wednesday 18 November 2015 14.30 AEDT\\n\\n', u'New Zealand is mourning one of its greatest sporting heroes after the former All Blacks player Jonah Lomu died unexpectedly in Auckland on Wednesday at the age of 40.', u' John Mayhew, the former All Blacks doctor, confirmed the news on Wednesday morning.', u'\\u201cOn behalf of the Lomu family, I can confirm that Jonah Lomu died this morning, most probably about eight or nine this morning,\\u201d Mayhew said. \\u201cThe family are obviously devastated, as are friends and acquaintances.']\n"
     ]
    }
   ],
   "source": [
    "url='http://www.theguardian.com/sport/2015/nov/18/jonah-lomu-dies-aged-40-according-to-reports'\n",
    "raw=urlopen(url).read()\n",
    "soup=BeautifulSoup(raw,'html.parser')\n",
    "texts=[]\n",
    "for para in soup.find_all('p'):\n",
    "    text=para.text\n",
    "    texts.append(text)\n",
    "print texts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting slate\n",
      "/opt/python/lib/python2.7/site-packages/pip/_vendor/requests/packages/urllib3/util/ssl_.py:90: InsecurePlatformWarning: A true SSLContext object is not available. This prevents urllib3 from configuring SSL appropriately and may cause certain SSL connections to fail. For more information, see https://urllib3.readthedocs.org/en/latest/security.html#insecureplatformwarning.\n",
      "  InsecurePlatformWarning\n",
      "  Downloading slate-0.3.zip\n",
      "Collecting distribute (from slate)\n",
      "  Downloading distribute-0.7.3.zip (145kB)\n",
      "\u001b[K    100% |################################| 147kB 2.1MB/s \n",
      "\u001b[?25hRequirement already satisfied (use --upgrade to upgrade): setuptools>=0.7 in /opt/python/lib/python2.7/site-packages (from distribute->slate)\n",
      "Building wheels for collected packages: slate, distribute\n",
      "  Running setup.py bdist_wheel for slate\n",
      "  Stored in directory: /home/researcher/.cache/pip/wheels/20/fc/87/c8e15d85f2505dc89a72773801d69ee10fa33785c3c7f6a3e7\n",
      "  Running setup.py bdist_wheel for distribute\n",
      "  Stored in directory: /home/researcher/.cache/pip/wheels/b2/3c/64/772be880a32a0c41e64b56b13c25450ff31cf363670d3bc576\n",
      "Successfully built slate distribute\n",
      "Installing collected packages: distribute, slate\n",
      "Successfully installed distribute-0.7.3 slate-0.3\n"
     ]
    }
   ],
   "source": [
    "!pip install slate #install slate for PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2015-11-18 04:28:04--  http://www.planetebook.com/ebooks/1984.pdf\n",
      "Resolving www.planetebook.com (www.planetebook.com)... 69.194.236.162\n",
      "Connecting to www.planetebook.com (www.planetebook.com)|69.194.236.162|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1366706 (1.3M) [application/pdf]\n",
      "Saving to: '1984.pdf.1'\n",
      "\n",
      "100%[======================================>] 1,366,706    294KB/s   in 5.8s   \n",
      "\n",
      "2015-11-18 04:28:10 (230 KB/s) - '1984.pdf.1' saved [1366706/1366706]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://www.planetebook.com/ebooks/1984.pdf\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import slate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1984.pdf  1984.pdf.1  books  synbiotokens  synbiotokens.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Download free eBooks of classic literature, books and novels at Planet eBook. Subscribe to our free eBooks blog and email newsletter.1984By George Orwell\\x0c'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('1984.pdf') as f:\n",
    "    doc = slate.PDF(f)\n",
    "doc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "slate.slate.PDF"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
